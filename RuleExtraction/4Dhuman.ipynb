{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install 4D human requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '4D-Humans'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: 4D-humans\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\docto\\appdata\\roaming\\python\\python311\\site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///C:/BU/CSSE/masterthesis/ChatFeeder/RuleExtraction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n",
      "ERROR: file:///C:/BU/CSSE/masterthesis/ChatFeeder/RuleExtraction does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/shubham-goel/4D-Humans.git\n",
    "!conda create --name 4D-humans python=3.10\n",
    "!conda activate 4D-humans\n",
    "!pip install torch\n",
    "!cd 4D-Humans\n",
    "!pip install -e .[all]\n",
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/brjathu/PHALP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4DHuman\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "import numpy as np\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from phalp.configs.base import FullConfig\n",
    "from phalp.models.hmar.hmr import HMR2018Predictor\n",
    "from phalp.trackers.PHALP import PHALP\n",
    "from phalp.utils import get_pylogger\n",
    "from phalp.configs.base import CACHE_DIR\n",
    "\n",
    "from 4DHumans.hmr2.datasets.utils import expand_bbox_to_aspect_ratio\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "log = get_pylogger(__name__)\n",
    "\n",
    "class HMR2Predictor(HMR2018Predictor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg)\n",
    "        # Setup our new model\n",
    "        from hmr2.models import download_models, load_hmr2\n",
    "\n",
    "        # Download and load checkpoints\n",
    "        download_models()\n",
    "        model, _ = load_hmr2()\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hmar_out = self.hmar_old(x)\n",
    "        batch = {\n",
    "            'img': x[:,:3,:,:],\n",
    "            'mask': (x[:,3,:,:]).clip(0,1),\n",
    "        }\n",
    "        model_out = self.model(batch)\n",
    "        out = hmar_out | {\n",
    "            'pose_smpl': model_out['pred_smpl_params'],\n",
    "            'pred_cam': model_out['pred_cam'],\n",
    "        }\n",
    "        return out\n",
    "    \n",
    "class HMR2023TextureSampler(HMR2Predictor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # Model's all set up. Now, load tex_bmap and tex_fmap\n",
    "        # Texture map atlas\n",
    "        bmap = np.load(os.path.join(CACHE_DIR, 'phalp/3D/bmap_256.npy'))\n",
    "        fmap = np.load(os.path.join(CACHE_DIR, 'phalp/3D/fmap_256.npy'))\n",
    "        self.register_buffer('tex_bmap', torch.tensor(bmap, dtype=torch.float))\n",
    "        self.register_buffer('tex_fmap', torch.tensor(fmap, dtype=torch.long))\n",
    "\n",
    "        self.img_size = 256         #self.cfg.MODEL.IMAGE_SIZE\n",
    "        self.focal_length = 5000.   #self.cfg.EXTRA.FOCAL_LENGTH\n",
    "\n",
    "        import neural_renderer as nr\n",
    "        self.neural_renderer = nr.Renderer(dist_coeffs=None, orig_size=self.img_size,\n",
    "                                          image_size=self.img_size,\n",
    "                                          light_intensity_ambient=1,\n",
    "                                          light_intensity_directional=0,\n",
    "                                          anti_aliasing=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = {\n",
    "            'img': x[:,:3,:,:],\n",
    "            'mask': (x[:,3,:,:]).clip(0,1),\n",
    "        }\n",
    "        model_out = self.model(batch)\n",
    "\n",
    "        # from hmr2.models.prohmr_texture import unproject_uvmap_to_mesh\n",
    "\n",
    "        def unproject_uvmap_to_mesh(bmap, fmap, verts, faces):\n",
    "            # bmap:  256,256,3\n",
    "            # fmap:  256,256\n",
    "            # verts: B,V,3\n",
    "            # faces: F,3\n",
    "            valid_mask = (fmap >= 0)\n",
    "\n",
    "            fmap_flat = fmap[valid_mask]      # N\n",
    "            bmap_flat = bmap[valid_mask,:]    # N,3\n",
    "\n",
    "            face_vids = faces[fmap_flat, :]  # N,3\n",
    "            face_verts = verts[:, face_vids, :] # B,N,3,3\n",
    "\n",
    "            bs = face_verts.shape\n",
    "            map_verts = torch.einsum('bnij,ni->bnj', face_verts, bmap_flat) # B,N,3\n",
    "\n",
    "            return map_verts, valid_mask\n",
    "\n",
    "        pred_verts = model_out['pred_vertices'] + model_out['pred_cam_t'].unsqueeze(1)\n",
    "        device = pred_verts.device\n",
    "        face_tensor = torch.tensor(self.smpl.faces.astype(np.int64), dtype=torch.long, device=device)\n",
    "        map_verts, valid_mask = unproject_uvmap_to_mesh(self.tex_bmap, self.tex_fmap, pred_verts, face_tensor) # B,N,3\n",
    "\n",
    "        # Project map_verts to image using K,R,t\n",
    "        # map_verts_view = einsum('bij,bnj->bni', R, map_verts) + t # R=I t=0\n",
    "        focal = self.focal_length / (self.img_size / 2)\n",
    "        map_verts_proj = focal * map_verts[:, :, :2] / map_verts[:, :, 2:3] # B,N,2\n",
    "        map_verts_depth = map_verts[:, :, 2] # B,N\n",
    "\n",
    "        # Render Depth. Annoying but we need to create this\n",
    "        K = torch.eye(3, device=device)\n",
    "        K[0, 0] = K[1, 1] = self.focal_length\n",
    "        K[1, 2] = K[0, 2] = self.img_size / 2  # Because the neural renderer only support squared images\n",
    "        K = K.unsqueeze(0)\n",
    "        R = torch.eye(3, device=device).unsqueeze(0)\n",
    "        t = torch.zeros(3, device=device).unsqueeze(0)\n",
    "        rend_depth = self.neural_renderer(pred_verts,\n",
    "                                        face_tensor[None].expand(pred_verts.shape[0], -1, -1).int(),\n",
    "                                        # textures=texture_atlas_rgb,\n",
    "                                        mode='depth',\n",
    "                                        K=K, R=R, t=t)\n",
    "\n",
    "        rend_depth_at_proj = torch.nn.functional.grid_sample(rend_depth[:,None,:,:], map_verts_proj[:,None,:,:]) # B,1,1,N\n",
    "        rend_depth_at_proj = rend_depth_at_proj.squeeze(1).squeeze(1) # B,N\n",
    "\n",
    "        img_rgba = torch.cat([batch['img'], batch['mask'][:,None,:,:]], dim=1) # B,4,H,W\n",
    "        img_rgba_at_proj = torch.nn.functional.grid_sample(img_rgba, map_verts_proj[:,None,:,:]) # B,4,1,N\n",
    "        img_rgba_at_proj = img_rgba_at_proj.squeeze(2) # B,4,N\n",
    "\n",
    "        visibility_mask = map_verts_depth <= (rend_depth_at_proj + 1e-4) # B,N\n",
    "        img_rgba_at_proj[:,3,:][~visibility_mask] = 0\n",
    "\n",
    "        # Paste image back onto square uv_image\n",
    "        uv_image = torch.zeros((batch['img'].shape[0], 4, 256, 256), dtype=torch.float, device=device)\n",
    "        uv_image[:, :, valid_mask] = img_rgba_at_proj\n",
    "\n",
    "        out = {\n",
    "            'uv_image':  uv_image,\n",
    "            'uv_vector' : self.hmar_old.process_uv_image(uv_image),\n",
    "            'pose_smpl': model_out['pred_smpl_params'],\n",
    "            'pred_cam':  model_out['pred_cam'],\n",
    "        }\n",
    "        return out\n",
    "\n",
    "class HMR2_4dhuman(PHALP):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "    def setup_hmr(self):\n",
    "        self.HMAR = HMR2023TextureSampler(self.cfg)\n",
    "\n",
    "    def get_detections(self, image, frame_name, t_, additional_data=None, measurments=None):\n",
    "        (\n",
    "            pred_bbox, pred_bbox, pred_masks, pred_scores, pred_classes, \n",
    "            ground_truth_track_id, ground_truth_annotations\n",
    "        ) =  super().get_detections(image, frame_name, t_, additional_data, measurments)\n",
    "\n",
    "        # Pad bounding boxes \n",
    "        pred_bbox_padded = expand_bbox_to_aspect_ratio(pred_bbox, self.cfg.expand_bbox_shape)\n",
    "\n",
    "        return (\n",
    "            pred_bbox, pred_bbox_padded, pred_masks, pred_scores, pred_classes,\n",
    "            ground_truth_track_id, ground_truth_annotations\n",
    "        )\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Human4DConfig(FullConfig):\n",
    "    # override defaults if needed\n",
    "    expand_bbox_shape: Optional[Tuple[int]] = (192,256)\n",
    "    pass\n",
    "\n",
    "cs = ConfigStore.instance()\n",
    "cs.store(name=\"config\", node=Human4DConfig)\n",
    "\n",
    "@hydra.main(version_base=\"1.2\", config_name=\"config\")\n",
    "def main(cfg: DictConfig) -> Optional[float]:\n",
    "    \"\"\"Main function for running the PHALP tracker.\"\"\"\n",
    "\n",
    "    phalp_tracker = HMR2_4dhuman(cfg)\n",
    "\n",
    "    phalp_tracker.track()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
